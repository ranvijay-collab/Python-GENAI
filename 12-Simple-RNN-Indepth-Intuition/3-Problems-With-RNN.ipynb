{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems With RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A Gated Recurrent Unit (GRU) \n",
    "2. GRU RNN (A Gated Recurrent Unit (GRU))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Gated Recurrent Unit (GRU)\n",
    "\n",
    "Long Short-Term Memory (LSTM) is a type of Recurrent Neural Network (RNN) that uses gates to store and process information over multiple time steps\n",
    "\n",
    "LSTMs are used to analyze sequential data, such as time series, speech, and text. They are known for their ability to handle long-term dependencies in data, and are used in applications like speech recognition, natural language processing, and time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs use three gates to control the flow of information into and out of the cell:\n",
    "1. Input gate: Controls what information is added to the memory cell \n",
    "2. Forget gate: Controls what information is removed from the memory cell \n",
    "3. Output gate: Controls what information is output from the memory cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU RNN (A Gated Recurrent Unit (GRU))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gated Recurrent Unit (GRU) is a type of recurrent neural network (RNN) that's used to process sequential data. GRUs are designed to selectively remember or forget information over time, and are often used when speed is important for processing large amounts of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some things to know about GRUs:\n",
    "1. Architecture: GRUs have a simpler structure than other RNNs, like Long Short-Term Memory (LSTM) networks, with only \n",
    "2. two gates: the update gate and the reset gate. \n",
    "3. Speed: GRUs are faster than LSTMs. \n",
    "4. Memory: GRUs use less memory than LSTMs. \n",
    "5. Training: GRUs are easier to train than LSTMs because of their simpler architecture. \n",
    "Vanishing gradient problem: GRUs address the vanishing gradient problem, which can cause RNNs to \"forget\" longer sequences. \n",
    "6. Training capabilities: GRUs can be trained to keep information from long ago, remove irrelevant information, and perform well in complex scenarios"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
