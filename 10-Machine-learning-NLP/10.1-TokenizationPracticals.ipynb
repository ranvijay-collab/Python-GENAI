{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Tokenization and Basic Terminologies\n",
    "\t\n",
    "ANS: Converting the Paragraph or sentances in to a token called Tokenization\n",
    "  ex: 1. If we are convertinf the paragraph to sentances it is a token\n",
    "      2. if we are converting the sentances to words It is a Token\n",
    "\n",
    "\n",
    "  ### -Tokenization in NLP                                         \n",
    "\t  ##### Topics============\n",
    "        1.Corps        -> Paragraph\n",
    "        2.Documents \t-> Sentances\n",
    "        3.Vacabulery -> Unique Word\n",
    "        4.Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### ====Tokenization===\n",
    "   [I like to drink apple juice. My friend like Mango Juice] -------Tokenization\n",
    "   \n",
    "   ### ===Words==\n",
    "    1.I like to drink apple juice\n",
    "\t2.My friend likes Mango Juice\n",
    "\t\n",
    "   ### ==Vacabulery==\n",
    "    Above Sentances have the 10 Unique words that is call Vacabulery like,[I like,to, drink, apple, juice,My, friend,likes,Mango]\n",
    "\t\n",
    "#### Q Explore bellow Topics \n",
    "  -- spacie\n",
    "  --Natural Language Toolkit(NLTK):\n",
    "\n",
    "   https://www.nltk.org/\n",
    "\tNatural Language Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome,to Krish Naik's NLP Tutorials.\n",
      "Please do watch the entire course! to become expert in NLP.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus=\"\"\"Hello Welcome,to Krish Naik's NLP Tutorials.\n",
    "Please do watch the entire course! to become expert in NLP.\n",
    "\"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\HP/nltk_data'\n",
      "    - 'c:\\\\Users\\\\HP\\\\GEN-AI-COURSE\\\\Python\\\\venv\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\HP\\\\GEN-AI-COURSE\\\\Python\\\\venv\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\HP\\\\GEN-AI-COURSE\\\\Python\\\\venv\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\HP\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Tokenization\n",
    "## Sentances-paragraphs\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download the punkt tokenizer model\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define the corpus\n",
    "corpus = \"\"\"Hello Welcome, to Krish Naik's NLP Tutorials.\n",
    "Please do watch the entire course! to become expert in NLP.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    sentences = sent_tokenize(corpus,language=\"english\")\n",
    "    print(sentences)\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'to', 'Krish', 'Naik', \"'s\", 'NLP', 'Tutorials', '.', 'Please', 'do', 'watch', 'the', 'entire', 'course', '!', 'to', 'become', 'expert', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "## Tokenization\n",
    "## paragraph--->words\n",
    "## sentence---->words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "corpus = \"\"\"Hello Welcome, to Krish Naik's NLP Tutorials.\n",
    "Please do watch the entire course! to become expert in NLP.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    word_doccument= word_tokenize(corpus)\n",
    "    print(word_doccument)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'to', 'Krish', 'Naik', \"'\", 's', 'NLP', 'Tutorials', '.', 'Please', 'do', 'watch', 'the', 'entire', 'course', '!', 'to', 'become', 'expert', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "## Tokenization\n",
    "## paragraph--->words\n",
    "## sentence---->words\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "corpus = \"\"\"Hello Welcome, to Krish Naik's NLP Tutorials.\n",
    "Please do watch the entire course! to become expert in NLP.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    word_doccument= wordpunct_tokenize(corpus)\n",
    "    print(word_doccument)\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'to', 'Krish', 'Naik', \"'s\", 'NLP', 'Tutorials.', 'Please', 'do', 'watch', 'the', 'entire', 'course', '!', 'to', 'become', 'expert', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "corpus = \"\"\"Hello Welcome, to Krish Naik's NLP Tutorials.\n",
    "Please do watch the entire course! to become expert in NLP.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    word_doccument= tokenizer.tokenize(corpus)\n",
    "    print(word_doccument)\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
